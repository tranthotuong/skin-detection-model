{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Aa36bMKLze3z"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","import cv2\n","from keras import backend as K\n","from keras.layers import Layer,InputSpec\n","import keras.layers as kl\n","from glob import glob\n","from sklearn.metrics import roc_curve, auc\n","from keras.preprocessing import image\n","from tensorflow.keras.models import Sequential\n","from sklearn.metrics import roc_auc_score\n","from tensorflow.keras import callbacks,regularizers\n","from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n","from  matplotlib import pyplot as plt\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import concatenate,Dense, Conv2D, MaxPooling2D, Flatten,Input,Activation,add,AveragePooling2D,GlobalAveragePooling2D,BatchNormalization,Dropout\n","%matplotlib inline\n","import shutil\n","from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n","from tensorflow.python.platform import build_info as tf_build_info\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnzRzk7e44HL","outputId":"6d1e2d2f-1669-42a6-f6a7-45a52b73ce41"},"outputs":[],"source":["data_pd = pd.read_csv('/kaggle/input/original-dataset/Dataset/HAM10000/HAM10000_metadata.csv')\n","data_pd.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_pd = pd.read_csv('/kaggle/input/original-dataset/Dataset/HAM10000/ISIC2018_Task3_Test_GroundTruth.csv')\n","test_pd.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IFqPgUu5jPj","outputId":"979e5d51-8319-435c-f4db-1981b452c849"},"outputs":[],"source":["df_count = data_pd.groupby('lesion_id').count()\n","df_count.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjMQNZRI2xl7"},"outputs":[],"source":["df_count = df_count[df_count['dx'] == 1]\n","df_count.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_count = pd.read_csv('/kaggle/input/ham10000-split15/HAM10000_DF/df_count.csv')\n","train_df = pd.read_csv('/kaggle/input/ham10000-split15/HAM10000_DF/train_df.csv')\n","vali_df = pd.read_csv('/kaggle/input/ham10000-split15/HAM10000_DF/vali_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ja7jQJQb39wi"},"outputs":[],"source":["# Image id of train and test images\n","train_list = list(train_df['image_id'])\n","vali_list = list(vali_df['image_id'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_list = list(test_pd['image_id'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBJgBAjP13q5","outputId":"463d1b3f-d77c-49b0-d89e-9ce5d21a47e0"},"outputs":[],"source":["len(vali_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(test_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEChk1DK-H8Z","outputId":"612e1f17-9db3-4dea-c99e-0b7aa5ee566a"},"outputs":[],"source":["len(train_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIoMqylGAYYZ"},"outputs":[],"source":["# Set the image_id as the index in data_pd\n","data_pd.set_index('image_id', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set the image_id as the index in data_pd\n","test_pd.set_index('image_id', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsoqCvNsgmHP"},"outputs":[],"source":["targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNisha_gM3_Z"},"outputs":[],"source":["train_path = '/kaggle/input/ham10000-split45/HAM10000_IRV2_SPLIT45/train_dir_IRv2'\n","vali_path = '/kaggle/input/ham10000-split45/HAM10000_IRV2_SPLIT45/vali_dir_IRv2'\n","test_path = '/kaggle/input/ham10000-split45/HAM10000_IRV2_SPLIT45/test_dir_IRv2'\n","batch_size = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhQWqdRN79B3"},"outputs":[],"source":["datagen=ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_resnet_v2.preprocess_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9_8FvOO7Rtu","outputId":"7ca88c24-fda8-45d6-cb32-f3a43ae6ba5b"},"outputs":[],"source":["image_size = 299\n","print(\"\\nTrain Batches: \")\n","train_batches = datagen.flow_from_directory(directory=train_path,\n","                                            target_size=(image_size,image_size),\n","                                            batch_size=batch_size,\n","                                            shuffle=True)\n","\n","print(\"\\nVali Batches: \")\n","vali_batches =datagen.flow_from_directory(vali_path,\n","                                           target_size=(image_size,image_size),\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","print(\"\\nTest Batches: \")\n","test_batches =datagen.flow_from_directory(test_path,\n","                                           target_size=(image_size,image_size),\n","                                           batch_size=batch_size,\n","                                           shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbwfHcsOPKYB"},"outputs":[],"source":["#Soft Attention\n","\n","from keras import backend as K\n","from keras.layers import Layer,InputSpec\n","import keras.layers as kl\n","import tensorflow as tf\n","\n","\n","\n","class SoftAttention(Layer):\n","    def __init__(self,ch,m,concat_with_x=False,aggregate=False,**kwargs):\n","        self.channels=int(ch)\n","        self.multiheads = m\n","        self.aggregate_channels = aggregate\n","        self.concat_input_with_scaled = concat_with_x\n","\n","        \n","        super(SoftAttention,self).__init__(**kwargs)\n","\n","    def build(self,input_shape):\n","\n","        self.i_shape = input_shape\n","\n","        kernel_shape_conv3d = (self.channels, 3, 3) + (1, self.multiheads) # DHWC\n","    \n","        self.out_attention_maps_shape = input_shape[0:1]+(self.multiheads,)+input_shape[1:-1]\n","        \n","        if self.aggregate_channels==False:\n","\n","            self.out_features_shape = input_shape[:-1]+(input_shape[-1]+(input_shape[-1]*self.multiheads),)\n","        else:\n","            if self.concat_input_with_scaled:\n","                self.out_features_shape = input_shape[:-1]+(input_shape[-1]*2,)\n","            else:\n","                self.out_features_shape = input_shape\n","        \n","\n","        self.kernel_conv3d = self.add_weight(shape=kernel_shape_conv3d,\n","                                        initializer='he_uniform',\n","                                        name='kernel_conv3d')\n","        self.bias_conv3d = self.add_weight(shape=(self.multiheads,),\n","                                      initializer='zeros',\n","                                      name='bias_conv3d')\n","\n","        super(SoftAttention, self).build(input_shape)\n","\n","    def call(self, x):\n","\n","        exp_x = K.expand_dims(x,axis=-1)\n","\n","        c3d = K.conv3d(exp_x,\n","                     kernel=self.kernel_conv3d,\n","                     strides=(1,1,self.i_shape[-1]), padding='same', data_format='channels_last')\n","        conv3d = K.bias_add(c3d,\n","                        self.bias_conv3d)\n","        conv3d = kl.Activation('relu')(conv3d)\n","\n","        conv3d = K.permute_dimensions(conv3d,pattern=(0,4,1,2,3))\n","\n","        \n","        conv3d = K.squeeze(conv3d, axis=-1)\n","        conv3d = K.reshape(conv3d,shape=(-1, self.multiheads ,self.i_shape[1]*self.i_shape[2]))\n","\n","        softmax_alpha = K.softmax(conv3d, axis=-1) \n","        softmax_alpha = kl.Reshape(target_shape=(self.multiheads, self.i_shape[1],self.i_shape[2]))(softmax_alpha)\n","\n","        \n","        if self.aggregate_channels==False:\n","            exp_softmax_alpha = K.expand_dims(softmax_alpha, axis=-1)       \n","            exp_softmax_alpha = K.permute_dimensions(exp_softmax_alpha,pattern=(0,2,3,1,4))\n","   \n","            x_exp = K.expand_dims(x,axis=-2)\n","   \n","            u = kl.Multiply()([exp_softmax_alpha, x_exp])   \n","  \n","            u = kl.Reshape(target_shape=(self.i_shape[1],self.i_shape[2],u.shape[-1]*u.shape[-2]))(u)\n","\n","        else:\n","            exp_softmax_alpha = K.permute_dimensions(softmax_alpha,pattern=(0,2,3,1))\n","\n","            exp_softmax_alpha = K.sum(exp_softmax_alpha,axis=-1)\n","\n","            exp_softmax_alpha = K.expand_dims(exp_softmax_alpha, axis=-1)\n","\n","            u = kl.Multiply()([exp_softmax_alpha, x])   \n","\n","        if self.concat_input_with_scaled:\n","            o = kl.Concatenate(axis=-1)([u,x])\n","        else:\n","            o = u\n","        \n","        return [o, softmax_alpha]\n","\n","    def compute_output_shape(self, input_shape): \n","        return [self.out_features_shape, self.out_attention_maps_shape]\n","\n","    \n","    def get_config(self):\n","        return super(SoftAttention,self).get_config()\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrlJwba5By1A","outputId":"4112378c-a50b-4c15-a9f1-f326c4858b8b"},"outputs":[],"source":["irv2 = tf.keras.applications.InceptionResNetV2(\n","    include_top=True,\n","    weights=\"imagenet\",\n","    input_tensor=None,\n","    input_shape=None,\n","    pooling=None,\n","    classifier_activation=\"softmax\",\n","\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for i, layer in enumerate(resnet.layers):\n","#     layer.trainable = False\n","#     print(f'Layer {i}: {layer.name} - Trainable: {layer.trainable}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["irv2.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#-152, -495, -742\n","for layer in irv2.layers[-495:]:\n","    layer.trainable = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Exclude the last 28 layers of the model.\n","conv = irv2.layers[-28].output"]},{"cell_type":"markdown","metadata":{"id":"eTFfWfqXVjsf"},"source":["Soft Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exrVTX_uVYPi"},"outputs":[],"source":["\n","\n","attention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(conv.shape[-1]),name='soft_attention')(conv)\n","attention_layer=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer))\n","conv=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(conv))\n","\n","conv = concatenate([conv,attention_layer])\n","conv  = Activation('relu')(conv)\n","conv = Dropout(0.5)(conv)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output = Flatten()(conv)\n","output = Dense(2048, activation='relu', kernel_regularizer=regularizers.l1(0.01))(output)\n","output = BatchNormalization()(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(output)\n","output = BatchNormalization()(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R13YR5JxVpOg"},"outputs":[],"source":["output = Dense(7, activation='softmax')(output)\n","model = Model(inputs=irv2.input, outputs=output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BD2GGKGNV23W","outputId":"4cd0da4e-f481-4c16-f387-b04a26870f3a"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WR0fUpy18vAZ"},"outputs":[],"source":["opt1=tf.keras.optimizers.Adam(learning_rate=0.001,epsilon=0.01)\n","model.compile(optimizer=opt1,\n","             loss='categorical_crossentropy',\n","             metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAf5ha295reS"},"outputs":[],"source":["class_weights = {   \n","                    0: 1,  # akiec 1\n","                    1: 1,  # bcc 1\n","                    2: 1,  # bkl 1\n","                    3: 1,  # df 1 \n","                    4: 5,  # mel 5\n","                    5: 1,  # nv 1\n","                    6: 1,  # vasc 1\n","                }\n","\n","\n","checkpoint=  ModelCheckpoint(filepath = 'IRV2_SA_TEST_DTS45_FZZ_BLKAnBnC_1501.hdf5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras import models\n","model.load_weights(\"/kaggle/working/IRV2_SA_TEST_DTS45_FZZ_BLKBnC_1401.hdf5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUzTmiZ-8hL3","outputId":"a5667822-7ace-49da-f26b-896db54c720d"},"outputs":[],"source":["Earlystop = EarlyStopping(monitor='val_loss', mode='min',patience=30, min_delta=0.001)\n","with tf.device('/gpu'):\n","    history = model.fit(train_batches,\n","                    steps_per_epoch=(len(train_df)/10),\n","                    epochs=150,\n","                    verbose=2,\n","                    validation_data=vali_batches,validation_steps=len(vali_df)/batch_size,callbacks=[checkpoint,Earlystop],class_weight=class_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zm_AewFBXTj8"},"outputs":[],"source":["from tensorflow.keras import models\n","model.load_weights(\"/kaggle/working/IRV2_SA_TEST_DTS45_FZZ_BLKAnBnC_15011.hdf5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = model.predict(test_batches, steps=len(test_pd)/batch_size, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYxCDDjusR-S","outputId":"dfb18a18-24d1-4093-9d9a-5ded9a857aa5"},"outputs":[],"source":["#geting predictions on test dataset\n","y_pred = np.argmax(predictions, axis=1)\n","targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n","#getting the true labels per image \n","y_true = test_batches.classes\n","#getting the predicted labels per image \n","y_prob=predictions\n","from tensorflow.keras.utils import to_categorical\n","y_test = to_categorical(y_true)\n","\n","# Creating classification report \n","report = classification_report(y_true, y_pred, target_names=targetnames)\n","\n","print(\"\\nClassification Report:\")\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Confusion Matrix - verify accuracy of each class\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","cm = confusion_matrix(y_true, y_pred)\n","#print(cm)\n","sns.heatmap(cm, annot=True)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"ResNet50+SA.ipynb","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4104122,"sourceId":7116474,"sourceType":"datasetVersion"},{"datasetId":4239406,"sourceId":7334497,"sourceType":"datasetVersion"},{"datasetId":4245287,"sourceId":7349245,"sourceType":"datasetVersion"},{"datasetId":4248754,"sourceId":7349248,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
